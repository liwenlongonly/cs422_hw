---
title: "CS 422"
output: 
  html_notebook:
    toc: yes
    toc_float: yes
author: Wenlong Li
---
### Part 2.1
```{r}
library(caret)
library(dplyr)
library(randomForest)
setwd("./")
df <- read.csv("hotel_bookings.csv", sep = ',',header = T)
df$is_canceled <- as.factor(df$is_canceled)

set.seed(1122)
index <- sample(1:nrow(df), 0.9*dim(df)[1])
train.df <- df[index,]
test.df <- df[-index,]

train.new.df <- select(train.df, -reservation_status, -country, -agent, -company, -reservation_status_date)

result <- matrix(nrow = 0, ncol = 7)
colnames(result) <- c("ntree","mtry","OOB", "Accuracy","BalancedAccuracy","Specificity","Sensitivity");

mtry = floor(sqrt(ncol(train.new.df)-1))

ntrees <- c(250, 500, 750)
mtrys <- c(mtry, mtry+1, mtry+2)
for(i in ntrees){
  for(j in mtrys){
    model <- randomForest(is_canceled ~ ., data=train.new.df,ntree=i, mtry=j, na.action = na.omit)
    oobError <- mean(model$err.rate)
    pred <- predict(model, test.df, type="class")
    cm <- confusionMatrix(pred, as.factor(test.df$is_canceled), positive='1')
    accuracy <- as.double(cm$overall["Accuracy"])
    balancedAccuracy <- as.double(cm$byClass["Balanced Accuracy"])
    specificity <- as.double(cm$byClass["Specificity"])
    sensitivity <- as.double(cm$byClass["Sensitivity"])
    result <- rbind(result, c(i, j, oobError, accuracy, balancedAccuracy, specificity, sensitivity))
  }
}
dt=data.frame(result)
```
### Part 2.1-a
```{r}
index <- which.max(unlist(dt$BalancedAccuracy))
ret <- dt[index,]
cat("Grid search resulted in the best model at ntree =",ret$ntree," and mtry =",ret$mtry,".\n",
    "Accuracy =",ret$Accuracy,"\n",
    "Balanced Accuracy =",ret$BalancedAccuracy,"\n",
    "Sensitivity =",ret$Sensitivity,"\n",
    "Specificity =",ret$Specificity)
```
### Part 2.1-b
```{r}
index <- which.min(unlist(dt$OOB))
ret <- dt[index,]
cat("Grid search resulted in the best model for OOB at ntree =",ret$ntree," and mtry =",ret$mtry,".\n",
   "OOB =",ret$OOB)
```
### Part 2.1-c
#### No, Mainly because of the problem of the verification set, the verification data of OOB Error is verification data for a certain decision tree, but it may be training data for another decision tree, so OOB Error cannot reflect the generalization ability of this model. From the output The data shows that the smallest OOB Error model has overfitting problems, so the cross-validation of OOB Error and confusion matrix cannot be used as the same evaluation indicator.