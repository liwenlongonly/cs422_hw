---
title: "Homework 7"
output: 
  html_notebook:
  toc: yes
  toc_float: yes
author: Wenlong Li
---
  
```{r}
library(keras)
library(dplyr)
library(caret)

rm(list=ls())

# Set working directory as needed
setwd("./")

df <- read.csv("activity-small.csv")

# Seed the PRNG
set.seed(1122)
df <- df[sample(nrow(df)), ] # Shuffle, as all of the data in the .csv file
                             # is ordered by label!  This will cause problems
                             # if we do not shuffle as the validation split
                             # may not include observations of class 3 (the
                             # class that occurs at the end).  The validation_
                             # split parameter samples from the end of the
                             # training set.

# Scale the dataset.  Copy this block of code as is and use it; we will get
# into the detail of why we scale.  We will scale our dataset so all of the
# predictors have a mean of 0 and standard deviation of 1.  Scale test and
# training splits independently!

indx <- sample(1:nrow(df), 0.20*nrow(df))
test.df  <- df[indx, ]
train.df <- df[-indx, ]

label.test <- test.df$label
test.df$label <- NULL
test.df <- as.data.frame(scale(test.df))
test.df$label <- label.test
rm(label.test)

label.train <- train.df$label
train.df$label <- NULL
train.df <- as.data.frame(scale(train.df))
train.df$label <- label.train
rm(label.train)
rm(indx)

```
### --- Your code goes below ---
### 2.1-a-i & ii
```{r}
create_model <- function(){
  model <- keras_model_sequential() %>%
  layer_dense(units = 8, activation="relu", input_shape=c(3)) %>%
  layer_dense(units = 4, activation="softmax")
  model %>% 
  compile(loss = "categorical_crossentropy", 
          optimizer="adam", 
          metrics=c("accuracy"))
  return(model)
}

X_train <- select(train.df, -label)
y_train <- train.df$label
y_train.ohe <- to_categorical(y_train)

X_test <- select(test.df, -label)
y_test <- test.df$label
y_test.ohe <- to_categorical(test.df$label)

model <- NULL
model <- create_model()

model %>% fit(
  data.matrix(X_train), 
  y_train.ohe,
  epochs=100,
  verbose=0,
  batch_size=1,
  validation_split=0.20
)

model %>% evaluate(as.matrix(X_test), y_test.ohe)
pred.prob <- predict(model, as.matrix(X_test))
pred.class <- apply(pred.prob, 1, function(x) which.max(x)-1)
cm <- confusionMatrix(as.factor(pred.class), as.factor(y_test))
str <- ""
for(n in 1:4){
  str <- paste0(str, "\nClass ",n,": Sens. = ",round(as.double(cm$byClass[n,"Sensitivity"]),3),", Spec. = ",round(as.double(cm$byClass[n,"Specificity"]),3),", Bal.Acc. = ",round(as.double(cm$byClass[n,"Balanced Accuracy"]),3))
}
cat("Batch gradient descent\n","Overall accuarcy:",round(as.double(cm$overall["Accuracy"]),3),str)

```
### 2.1-b
```{r}

batch.list<- c(1, 32, 64, 128, 256)

for (size in batch.list) {
  remove(model)
  model <- NULL
  model <- create_model()
  batch.list<- c(1, 32, 64, 128, 256)
  begin <- Sys.time()
  model %>% fit(
    data.matrix(X_train), 
    y_train.ohe,
    epochs=100,
    verbose=0,
    batch_size=size,
    validation_split=0.20
  )
  end <- Sys.time()
  
  model %>% evaluate(as.matrix(X_test), y_test.ohe)
  pred.prob <- predict(model, as.matrix(X_test))
  pred.class <- apply(pred.prob, 1, function(x) which.max(x)-1)
  cm <- confusionMatrix(as.factor(pred.class), as.factor(y_test))
  str <- ""
  for(n in 1:4){
    str <- paste0(str, "\nClass ",n,": Sens. = ",round(as.double(cm$byClass[n,"Sensitivity"]),3),", Spec. = ",round(as.double(cm$byClass[n,"Specificity"]),3),", Bal.Acc. = ",round(as.double(cm$byClass[n,"Balanced Accuracy"]),3))
  }
  cat("Batch size: ",size,"\nTime taken to train neural network:",end-begin,"(seconds)\n","Overall accuarcy:",round(as.double(cm$overall["Accuracy"]),3),str)
}
```

### 2.1-c-i
#### As the batch size becomes larger, the amount of data used for each training becomes smaller, and the time used for each training step becomes smaller.
### 2.1-c-ii
#### Change. As the batch size becomes larger, the amount of data used for each training becomes smaller, and the 100-step training cannot make the model converge, so the overall accuracy is reduced

### 2.1-d-i && ii
```{r}
create_new_model <- function(){
  model <- keras_model_sequential() %>%
  layer_dense(units = 8, activation="relu", input_shape=c(3)) %>%
  layer_dense(units = 8, activation="relu") %>%
  layer_dense(units = 4, activation="softmax")
  
  model %>% 
  compile(loss = "categorical_crossentropy", 
          optimizer="adam", 
          metrics=c("accuracy"))
  return(model)
}

remove(model)
model <- NULL
model <- create_new_model()

model %>% fit(
  data.matrix(X_train), 
  y_train.ohe,
  epochs=100,
  verbose=0,
  batch_size=1,
  validation_split=0.20
)

model %>% evaluate(as.matrix(X_test), y_test.ohe)
pred.prob <- predict(model, as.matrix(X_test))
pred.class <- apply(pred.prob, 1, function(x) which.max(x)-1)
cm <- confusionMatrix(as.factor(pred.class), as.factor(y_test))

str <- ""
for(n in 1:4){
  str <- paste0(str, "\nClass ",n,": Sens. = ",round(as.double(cm$byClass[n,"Sensitivity"]),3),", Spec. = ",round(as.double(cm$byClass[n,"Specificity"]),3),", Bal.Acc. = ",round(as.double(cm$byClass[n,"Balanced Accuracy"]),3))
}
cat("Batch gradient descent\n","Overall accuarcy:",round(as.double(cm$overall["Accuracy"]),3),str)

```

#### The performance increase
